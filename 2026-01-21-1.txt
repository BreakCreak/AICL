(AICL) root@4c91ca1b33d2:~/conda/share/doc/AICL# sh ./scripts/train.sh
Using device: cuda
[0.2778 0.2521 0.2264 0.1942 0.1508 0.1056 0.0620]
epoch=    3  step=   50  Loss=4.9291  cls_acc=46.67  best_map=18.13
[0.3691 0.3344 0.3010 0.2588 0.2006 0.1451 0.0834]
epoch=    7  step=  100  Loss=4.1472  cls_acc=59.05  best_map=24.18
[0.4279 0.3867 0.3421 0.2862 0.2237 0.1649 0.0942]
epoch=   11  step=  150  Loss=3.5957  cls_acc=62.38  best_map=27.51
[0.4962 0.4560 0.4055 0.3445 0.2760 0.2026 0.1225]
epoch=   15  step=  200  Loss=3.1485  cls_acc=67.14  best_map=32.90
[0.5344 0.4918 0.4381 0.3723 0.2949 0.2186 0.1333]
epoch=   19  step=  250  Loss=2.8657  cls_acc=73.81  best_map=35.48
[0.5412 0.4996 0.4461 0.3791 0.3010 0.2216 0.1321]
epoch=   23  step=  300  Loss=2.6189  cls_acc=73.33  best_map=36.01
[0.5507 0.5092 0.4546 0.3863 0.3083 0.2259 0.1349]
epoch=   26  step=  350  Loss=2.4267  cls_acc=74.29  best_map=36.71
[0.5469 0.5045 0.4516 0.3853 0.3062 0.2278 0.1384]
epoch=   30  step=  400  Loss=2.2793  cls_acc=73.81  best_map=36.71
[0.5990 0.5520 0.4922 0.4173 0.3302 0.2429 0.1452]
epoch=   34  step=  450  Loss=2.1745  cls_acc=80.95  best_map=39.70
[0.6003 0.5545 0.4962 0.4241 0.3370 0.2495 0.1493]
epoch=   38  step=  500  Loss=2.0803  cls_acc=80.00  best_map=40.16
[0.6129 0.5661 0.5041 0.4252 0.3387 0.2484 0.1487]
epoch=   42  step=  550  Loss=1.9841  cls_acc=82.38  best_map=40.63
[0.6211 0.5738 0.5121 0.4329 0.3451 0.2515 0.1483]
epoch=   46  step=  600  Loss=1.9387  cls_acc=81.43  best_map=41.21
[0.6232 0.5753 0.5118 0.4378 0.3537 0.2555 0.1548]
epoch=   49  step=  650  Loss=1.8645  cls_acc=82.38  best_map=41.60
[0.6243 0.5770 0.5160 0.4361 0.3516 0.2535 0.1500]
epoch=   53  step=  700  Loss=1.8337  cls_acc=79.52  best_map=41.60
[0.6374 0.5900 0.5247 0.4468 0.3582 0.2594 0.1574]
epoch=   57  step=  750  Loss=1.7945  cls_acc=81.90  best_map=42.48
[0.6321 0.5859 0.5217 0.4427 0.3557 0.2566 0.1561]
epoch=   61  step=  800  Loss=1.7306  cls_acc=80.95  best_map=42.48
[0.6399 0.5944 0.5312 0.4527 0.3601 0.2619 0.1615]
epoch=   65  step=  850  Loss=1.7079  cls_acc=82.38  best_map=42.88
[0.6441 0.5973 0.5348 0.4558 0.3657 0.2661 0.1597]
epoch=   69  step=  900  Loss=1.6704  cls_acc=83.33  best_map=43.19
[0.6461 0.5986 0.5330 0.4576 0.3690 0.2643 0.1630]
epoch=   73  step=  950  Loss=1.6441  cls_acc=81.90  best_map=43.31
[0.6456 0.5996 0.5350 0.4585 0.3691 0.2672 0.1590]
epoch=   76  step= 1000  Loss=1.6048  cls_acc=81.90  best_map=43.34
[0.6473 0.6009 0.5370 0.4622 0.3724 0.2665 0.1663]
epoch=   80  step= 1050  Loss=1.5973  cls_acc=83.33  best_map=43.61
[0.6504 0.6030 0.5380 0.4601 0.3700 0.2652 0.1631]
epoch=   84  step= 1100  Loss=1.5602  cls_acc=81.90  best_map=43.61
[0.6525 0.6051 0.5382 0.4604 0.3735 0.2658 0.1643]
epoch=   88  step= 1150  Loss=1.5256  cls_acc=85.71  best_map=43.71
[0.6461 0.6012 0.5376 0.4610 0.3714 0.2649 0.1611]
epoch=   92  step= 1200  Loss=1.5210  cls_acc=84.29  best_map=43.71
[0.6492 0.6047 0.5394 0.4631 0.3728 0.2652 0.1615]
epoch=   96  step= 1250  Loss=1.4920  cls_acc=84.76  best_map=43.71
[0.6457 0.6023 0.5399 0.4570 0.3658 0.2605 0.1570]
epoch=   99  step= 1300  Loss=1.4804  cls_acc=85.24  best_map=43.71
[0.6497 0.6068 0.5401 0.4612 0.3704 0.2640 0.1600]
epoch=  103  step= 1350  Loss=1.4734  cls_acc=85.24  best_map=43.71
[0.6530 0.6068 0.5405 0.4634 0.3726 0.2653 0.1610]
epoch=  107  step= 1400  Loss=1.4375  cls_acc=84.29  best_map=43.75
[0.6514 0.6089 0.5429 0.4661 0.3736 0.2672 0.1612]
epoch=  111  step= 1450  Loss=1.4247  cls_acc=84.29  best_map=43.88
[0.6519 0.6097 0.5430 0.4635 0.3724 0.2679 0.1609]
epoch=  115  step= 1500  Loss=1.4047  cls_acc=85.71  best_map=43.88
[0.6564 0.6109 0.5421 0.4650 0.3765 0.2655 0.1665]
epoch=  119  step= 1550  Loss=1.4110  cls_acc=84.29  best_map=44.04
[0.6551 0.6106 0.5431 0.4675 0.3751 0.2687 0.1616]
epoch=  123  step= 1600  Loss=1.3775  cls_acc=86.67  best_map=44.04
[0.6552 0.6123 0.5443 0.4650 0.3750 0.2691 0.1627]
epoch=  126  step= 1650  Loss=1.3677  cls_acc=85.24  best_map=44.05
[0.6574 0.6116 0.5435 0.4655 0.3763 0.2660 0.1677]
epoch=  130  step= 1700  Loss=1.3548  cls_acc=85.24  best_map=44.11
[0.6577 0.6126 0.5456 0.4685 0.3764 0.2672 0.1635]
epoch=  134  step= 1750  Loss=1.3500  cls_acc=86.19  best_map=44.16
[0.6546 0.6106 0.5429 0.4654 0.3730 0.2682 0.1616]
epoch=  138  step= 1800  Loss=1.3239  cls_acc=86.67  best_map=44.16
[0.6565 0.6129 0.5456 0.4661 0.3738 0.2694 0.1601]
epoch=  142  step= 1850  Loss=1.3300  cls_acc=86.19  best_map=44.16
[0.6571 0.6127 0.5452 0.4669 0.3744 0.2676 0.1639]
epoch=  146  step= 1900  Loss=1.3029  cls_acc=86.67  best_map=44.16
[0.6650 0.6194 0.5509 0.4723 0.3790 0.2698 0.1690]
epoch=  149  step= 1950  Loss=1.2963  cls_acc=85.71  best_map=44.65
[0.6606 0.6174 0.5497 0.4734 0.3813 0.2699 0.1621]
epoch=  153  step= 2000  Loss=1.2835  cls_acc=86.19  best_map=44.65
[0.6629 0.6174 0.5518 0.4710 0.3792 0.2697 0.1667]
epoch=  157  step= 2050  Loss=1.2791  cls_acc=85.71  best_map=44.65
[0.6553 0.6123 0.5452 0.4680 0.3733 0.2682 0.1589]
epoch=  161  step= 2100  Loss=1.2583  cls_acc=85.24  best_map=44.65
[0.6610 0.6168 0.5521 0.4709 0.3776 0.2683 0.1623]
epoch=  165  step= 2150  Loss=1.2517  cls_acc=85.24  best_map=44.65
[0.6601 0.6164 0.5511 0.4679 0.3765 0.2674 0.1602]
epoch=  169  step= 2200  Loss=1.2451  cls_acc=85.71  best_map=44.65
[0.6559 0.6130 0.5496 0.4681 0.3734 0.2647 0.1609]
epoch=  173  step= 2250  Loss=1.2403  cls_acc=85.71  best_map=44.65
[0.6565 0.6121 0.5498 0.4700 0.3736 0.2637 0.1593]
epoch=  176  step= 2300  Loss=1.2181  cls_acc=86.19  best_map=44.65
[0.6654 0.6196 0.5487 0.4673 0.3728 0.2651 0.1671]
epoch=  180  step= 2350  Loss=1.2158  cls_acc=84.76  best_map=44.65
[0.6630 0.6195 0.5555 0.4734 0.3767 0.2683 0.1614]
epoch=  184  step= 2400  Loss=1.2097  cls_acc=86.19  best_map=44.65
[0.6675 0.6222 0.5571 0.4731 0.3772 0.2687 0.1628]
epoch=  188  step= 2450  Loss=1.1838  cls_acc=85.71  best_map=44.69
[0.6611 0.6169 0.5538 0.4703 0.3734 0.2684 0.1613]
epoch=  192  step= 2500  Loss=1.1723  cls_acc=85.24  best_map=44.69
[0.6600 0.6143 0.5480 0.4664 0.3710 0.2662 0.1665]
epoch=  196  step= 2550  Loss=1.1769  cls_acc=83.33  best_map=44.69
[0.6668 0.6207 0.5566 0.4721 0.3747 0.2683 0.1595]
epoch=  199  step= 2600  Loss=1.1612  cls_acc=85.24  best_map=44.69
[0.6663 0.6208 0.5572 0.4751 0.3752 0.2665 0.1618]
epoch=  203  step= 2650  Loss=1.1562  cls_acc=86.19  best_map=44.69
[0.6649 0.6210 0.5568 0.4748 0.3731 0.2670 0.1613]
epoch=  207  step= 2700  Loss=1.1370  cls_acc=86.19  best_map=44.69
[0.6444 0.6027 0.5379 0.4605 0.3644 0.2545 0.1542]
epoch=  211  step= 2750  Loss=1.1309  cls_acc=84.29  best_map=44.69
[0.6642 0.6197 0.5569 0.4741 0.3737 0.2663 0.1598]
epoch=  215  step= 2800  Loss=1.1505  cls_acc=84.76  best_map=44.69
[0.6629 0.6192 0.5523 0.4704 0.3723 0.2636 0.1591]
epoch=  219  step= 2850  Loss=1.1219  cls_acc=85.24  best_map=44.69
[0.6615 0.6168 0.5532 0.4736 0.3728 0.2657 0.1588]
epoch=  223  step= 2900  Loss=1.1030  cls_acc=85.24  best_map=44.69
[0.6558 0.6126 0.5485 0.4645 0.3698 0.2600 0.1591]
epoch=  226  step= 2950  Loss=1.0993  cls_acc=86.19  best_map=44.69
[0.6621 0.6194 0.5567 0.4719 0.3727 0.2703 0.1633]
epoch=  230  step= 3000  Loss=1.0986  cls_acc=85.24  best_map=44.69
[0.6552 0.6128 0.5494 0.4683 0.3708 0.2612 0.1570]
epoch=  234  step= 3050  Loss=1.0993  cls_acc=85.71  best_map=44.69
[0.6562 0.6130 0.5505 0.4686 0.3709 0.2629 0.1565]
epoch=  238  step= 3100  Loss=1.0777  cls_acc=84.76  best_map=44.69
[0.6567 0.6131 0.5514 0.4681 0.3753 0.2632 0.1590]
epoch=  242  step= 3150  Loss=1.0843  cls_acc=86.19  best_map=44.69
[0.6590 0.6154 0.5530 0.4705 0.3709 0.2606 0.1570]
epoch=  246  step= 3200  Loss=1.0898  cls_acc=85.71  best_map=44.69
[0.6600 0.6162 0.5555 0.4717 0.3733 0.2660 0.1577]
epoch=  249  step= 3250  Loss=1.0649  cls_acc=86.19  best_map=44.69
[0.6492 0.6077 0.5473 0.4640 0.3674 0.2584 0.1546]
epoch=  253  step= 3300  Loss=1.0580  cls_acc=84.29  best_map=44.69
[0.6450 0.6043 0.5427 0.4596 0.3614 0.2556 0.1551]
epoch=  257  step= 3350  Loss=1.0506  cls_acc=81.43  best_map=44.69
[0.6453 0.6055 0.5408 0.4562 0.3640 0.2533 0.1525]
epoch=  261  step= 3400  Loss=1.0448  cls_acc=83.33  best_map=44.69
[0.6501 0.6065 0.5464 0.4630 0.3663 0.2557 0.1541]
epoch=  265  step= 3450  Loss=1.0380  cls_acc=82.86  best_map=44.69
[0.6542 0.6108 0.5500 0.4643 0.3678 0.2578 0.1587]
epoch=  269  step= 3500  Loss=1.0540  cls_acc=82.86  best_map=44.69
[0.6314 0.5897 0.5318 0.4474 0.3546 0.2473 0.1456]
epoch=  273  step= 3550  Loss=1.0354  cls_acc=83.33  best_map=44.69
[0.6545 0.6113 0.5462 0.4627 0.3696 0.2635 0.1574]
epoch=  276  step= 3600  Loss=1.0309  cls_acc=85.24  best_map=44.69
[0.6419 0.6006 0.5379 0.4554 0.3587 0.2523 0.1496]
epoch=  280  step= 3650  Loss=1.0247  cls_acc=82.38  best_map=44.69
[0.6417 0.6005 0.5396 0.4589 0.3633 0.2536 0.1537]
epoch=  284  step= 3700  Loss=1.0101  cls_acc=80.95  best_map=44.69
[0.6459 0.6043 0.5416 0.4571 0.3603 0.2506 0.1513]
epoch=  288  step= 3750  Loss=1.0174  cls_acc=83.33  best_map=44.69
[0.6445 0.6046 0.5441 0.4603 0.3647 0.2544 0.1570]
epoch=  292  step= 3800  Loss=1.0129  cls_acc=80.95  best_map=44.69
[0.6386 0.5977 0.5369 0.4565 0.3601 0.2536 0.1493]
epoch=  296  step= 3850  Loss=1.0039  cls_acc=81.90  best_map=44.69
[0.6489 0.6070 0.5457 0.4607 0.3649 0.2572 0.1575]
epoch=  299  step= 3900  Loss=0.9961  cls_acc=80.95  best_map=44.69
